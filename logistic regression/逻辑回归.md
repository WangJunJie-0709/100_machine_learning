# 逻辑回归

上一篇博客从数学原理和代码角度解释了线性回归，但是该模型只适合特征与预测值呈线性关系的回归场景，接下来我们将介绍逻辑回归，逻辑回归是一种用于二分类问题的机器学习算法。



## 数学原理







## 代码实战

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_breast_cancer
```

照常先导入所需要的包，我们这次使用的数据集是经典的乳腺癌数据集，该数据集包括了乳腺癌患者的不同特征。我们先大致看一下有哪一些特征

```python
# 加载乳腺癌数据集
data = load_breast_cancer()

print(data.feature_names)

# ['mean radius' 'mean texture' 'mean perimeter' 'mean area'
# 'mean smoothness' 'mean compactness' 'mean concavity'
# 'mean concave points' 'mean symmetry' 'mean fractal dimension'
# 'radius error' 'texture error' 'perimeter error' 'area error'
# 'smoothness error' 'compactness error' 'concavity error'
# 'concave points error' 'symmetry error' 'fractal dimension error'
# 'worst radius' 'worst texture' 'worst perimeter' 'worst area'
# 'worst smoothness' 'worst compactness' 'worst concavity'
# 'worst concave points' 'worst symmetry' 'worst fractal dimension']

print(data.target_names)

# ['malignant' 'benign']
```

数据集的特征包括以下几列（这个特征数还算少的，我们就先全部列出来，看一下是否符合，之后介绍的算法可能不会介绍全部的特征）

- mean radius：平均半径
- mean texture：平均纹理
- mean perimeter：平均周长
- mean area：平均面积
- mean smoothness：平均平滑度
- mean compactness：平均紧密度
- mean concavity：平均凹度
- mean concave points：平均凹点
- mean symmetry：平均对称性
- mean fractal dimension：平均分形维数
- radius error：半径误差
- texture error：纹理误差
- perimeter error：周长误差
- area error：面积误差
- smoothness error：光滑度误差
- compactness error：紧密度误差
- concavity error：凹度误差
- concave points error：凹点误差
- symmetry error：对称性误差
- fractal dimension error：分形维数误差
- worst radius：最坏半径
- worst texture：最坏纹理
- worst perimeter：最坏周长
- worst area：最坏面积
- worst smoothness：最坏光滑度
- worst compactness：最坏紧密度
- worst concavity：最坏凹度
- worst concave points：最坏凹点
- worst symmetry：最坏对称性
- worst fractal dimension：最坏分形误差

目标类别分别为

- malignant：恶性
- benign：良性

可以看到特征均是从半径、纹理、周长、面积、光滑度、紧密度、凹度、凹点、对称性和分形误差这几个角度展开的。目标为恶性和良性。也就是说这个问题是一个二分类问题，使用逻辑回归算法再合适不过了。

```python
# 特征选择
features = ['mean radius', 'mean texture', 'mean perimeter', 'mean area', 'mean smoothness']
target = 'target'

# 划分特征和目标变量
X = pd.DataFrame(data.data, columns=data.feature_names)[features]
y = data.target

# 数据集划分
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 特征缩放
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
```



```python
from sklearn.linear_model import LogisticRegression

# 创建逻辑回归模型
model = LogisticRegression()

# 模型训练
model.fit(X_train, y_train)

# 估计特征的权重
weights = model.coef_
intercept = model.intercept_
```



```python
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# 预测测试数据集
y_pred = model.predict(X_test)

# 计算评估指标
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 score:", f1)
```



```Python
import numpy as np

# 新样本特征
new_sample = np.array([15.12, 15.16, 97.67, 712.2, 0.0898]).reshape(1, -1)

# 特征缩放
new_sample_scaled = scaler.transform(new_sample)

# 预测新样本的分类
prediction = model.predict(new_sample_scaled)

print("Prediction:", prediction)
```









## 补充

上面的代码实战用到了几个评价指标，这里对这几个评价指标做一个简单的介绍，之后会专门对评价指标进行总结

### 评价指标

首先我们先看这样一个矩阵

|          | Positive | Negative |
| -------- | -------- | -------- |
| Positive | TP       | FP       |
| Negative | FN       | TN       |

上面这个矩阵也叫做混淆矩阵，行的positive、negative字段表示样本实际值，列的positive、negative字段表示预测的结果，那么我们就可以给出表格中值的几个概念

- TP：True Positive。预测为正确的正样本
- FP：False Positive。预测为错误的正样本
- TN：True Negative。预测为正确的负样本
- FN：True Negative。预测为正确的负样本

那么我们就可以解释代码中用到的几个指标

- 准确率Accuracy：分类器正确分类的样本数与总的样本数之比，即预测正确的概率
  $$
  Accuracy=\frac{TP+FN}{TP+FP+TN+FN}
  $$
  

- 精准率Precision：预测为正的样本中，实际为正样本的比例
  $$
  Pecision=\frac{TP}{TP+FP}
  $$
  

- 召回率Recall：实际为正的样本被预测为正样本的比例
  $$
  Recall=\frac{TP}{TP+FN}
  $$
  

- F1 Score：由于精准率和召回率是互相矛盾的，为了综合考虑这两个指标，再尽可能提高这两个指标的同时，也希望两者之间的差异尽可能小。
  $$
  F1=2\frac{P*R}{p+R}
  $$
  